{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c244e6",
   "metadata": {},
   "source": [
    "# Dataset Creation Using DataLad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6328d",
   "metadata": {},
   "source": [
    "Make sure you install datalad!  \n",
    "Once installed, run the box below to get the backbone of the dataset (not downloaded yet).  \n",
    "Don't forget to change user name in designated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5f6ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clone attempt:   0%|              | 0.00/2.00 [00:00<?, ? Candidate locations/s]\n",
      "Enumerating: 0.00 Objects [00:00, ? Objects/s]\u001b[A\n",
      "                                              \u001b[A\n",
      "Counting:   0%|                               | 0.00/3.00 [00:00<?, ? Objects/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Compressing:   0%|                            | 0.00/3.00 [00:00<?, ? Objects/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Receiving:   0%|                             | 0.00/48.6k [00:00<?, ? Objects/s]\u001b[A\n",
      "Receiving:   1%|▏                     | 487/48.6k [00:00<00:20, 2.31k Objects/s]\u001b[A\n",
      "Receiving:   5%|█                   | 2.43k/48.6k [00:00<00:14, 3.11k Objects/s]\u001b[A\n",
      "Receiving:  22%|████▍               | 10.7k/48.6k [00:00<00:08, 4.35k Objects/s]\u001b[A\n",
      "Receiving:  43%|████████▌           | 20.9k/48.6k [00:00<00:04, 5.75k Objects/s]\u001b[A\n",
      "Receiving:  48%|█████████▌          | 23.3k/48.6k [00:03<00:11, 2.18k Objects/s]\u001b[A\n",
      "Receiving:  52%|██████████▍         | 25.3k/48.6k [00:05<00:15, 1.50k Objects/s]\u001b[A\n",
      "Receiving:  81%|████████████████▏   | 39.4k/48.6k [00:05<00:04, 2.13k Objects/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "Resolving:   0%|                              | 0.00/7.79k [00:00<?, ? Deltas/s]\u001b[A\n",
      "Resolving:  65%|█████████████▋       | 5.06k/7.79k [00:00<00:00, 50.6k Deltas/s]\u001b[A\n",
      "Resolving:  91%|███████████████████  | 7.09k/7.79k [00:00<00:00, 30.0k Deltas/s]\u001b[A\n",
      "[INFO   ] scanning for annexed files (this may take some time)                  \u001b[A\n",
      "[INFO   ] Remote origin not usable by git-annex; setting annex-ignore \n",
      "[INFO   ] https://github.com/OpenNeuroDatasets/ds000113.git/config download failed: Not Found \n",
      "\u001b[1;1minstall\u001b[0m(\u001b[1;32mok\u001b[0m): /home/anakin/Desktop/ds000113 (\u001b[1;35mdataset\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import datalad.api as dl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "\n",
    "! datalad clone https://github.com/OpenNeuroDatasets/ds000113.git /home/anakin/Desktop/ds000113 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c36f5f",
   "metadata": {},
   "source": [
    "You can check the folder to understand how the dataset is orgenized (BIDS format).  \n",
    "For the purpose of the workshop we will download 2 runs for 8 subjects: subs 1-6 + 9-10, runs 3 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a2cae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset('/home/anakin/Desktop/ds000113')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the path to which the the dataset was initialized\n",
    "data_dir = r'/home/anakin/Desktop/ds000113'\n",
    "\n",
    "# Define sub list for download\n",
    "last_sub = 10\n",
    "excluded = [7,8]\n",
    "sub_list = [\"sub-{:02d}\".format(i) for i in range(1,last_sub+1) if i not in excluded]\n",
    "\n",
    "# creat a dataset object using datalad API\n",
    "ds = dl.Dataset(data_dir)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5433184",
   "metadata": {},
   "source": [
    "We will download the functional and anatomical data using a for loop.  \n",
    "This includes run-3 and run-7 functional files (including their json file) and the anatomical file.  \n",
    "To verify, you can check the directory and make sure downloaded files are now marked with a lock and not with a cross sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d5d299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sub in sub_list:\n",
    "    ds.get(glob.glob(os.path.join(data_dir,sub,'ses-movie','func',f'{sub}_ses-movie_task-movie_run-3_bold*')))\n",
    "    ds.get(glob.glob(os.path.join(data_dir,sub,'ses-movie','func',f'{sub}_ses-movie_task-movie_run-7_bold*')))\n",
    "    ds.get(glob.glob(os.path.join(data_dir,sub,'ses-forrestgump','anat',f'{sub}_ses-forrestgump_T1w.nii.gz')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb58d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also tcheck how much data we downloaded. uncomment the line below and run\n",
    "#ds.status(annex='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ab19ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'get',\n",
       "  'path': '/home/anakin/Desktop/ds000113/participants.tsv',\n",
       "  'type': 'file',\n",
       "  'refds': '/home/anakin/Desktop/ds000113',\n",
       "  'status': 'notneeded',\n",
       "  'message': 'already present'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get annotations, participants data and task json\n",
    "ds.get(glob.glob(os.path.join(data_dir,'stimuli','annotations','*')))\n",
    "ds.get(glob.glob(os.path.join(data_dir,'task-movie_bold.json')))\n",
    "ds.get(glob.glob(os.path.join(data_dir,'participants.tsv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d0690",
   "metadata": {},
   "source": [
    "Let's make a renamed copy (task = 'sync') of the data we downloaded:  \n",
    "- create a copy of functional data for selected runs (renamed to 'task-sync') for all subjects.  \n",
    "- create a copy of the anatomical file (renamed) in a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58498387",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [3,7] # original run numbers\n",
    "runs = [i for i in range(1,3)] # desired naming convention\n",
    "end_str = r\"_bold.nii.gz\"\n",
    "\n",
    "for sub in sub_list:\n",
    "    # anatomical file\n",
    "    os.makedirs(os.path.join(data_dir,sub,'ses-movie','anat')) # create a directory\n",
    "    anat_in = os.path.join(data_dir,sub,'ses-forrestgump','anat',sub +'_ses-forrestgump_T1w.nii.gz')\n",
    "    img = nb.load(anat_in)\n",
    "    anat_out = os.path.join(data_dir,sub,'ses-movie','anat',sub +'_T1w.nii.gz')\n",
    "    save = nb.save(img, anat_out)\n",
    "    for seg,run in zip(segments,runs):         \n",
    "    # functional files\n",
    "        func_in = os.path.join(data_dir,sub,'ses-movie','func',sub +'_ses-movie_task-movie_run-'+str(seg)+end_str)\n",
    "        img = nb.load(func_in)\n",
    "        func_out = os.path.join(data_dir,sub,'ses-movie','func',sub +'_task-sync_run-'+str(run)+end_str)\n",
    "        save = nb.save(img, func_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e226e",
   "metadata": {},
   "source": [
    "Note that the new files are not associated with the original datalad dataset (they do not have a lock on their icon) which is desired if we want to manipulate the file. \n",
    "For the purpose of this workshop we will not slice and dice the files themselves but if you ever need to do this - know that it is possible using Nibabel.\n",
    "See https://nipy.org/nibabel/nibabel_images.html#image-slicing \n",
    "\n",
    "    \n",
    "Another thing to be done is to copy our event files of both runs to every subject functional folder.  \n",
    "Since I modeled mutual events (percieved audiovisual features and social synchrony), the event file is the same for all subjects.  \n",
    "\n",
    "In case you are running your own paradigm - it is highly recommended that you create a tsv file in the same for the experimental events in the same format as displayed here (4 columns named 'onset','duration','weight' and 'stimulus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95b5ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1_data = pd.read_csv('data/sync_run-1_events.tsv',delimiter='\\t')\n",
    "run2_data = pd.read_csv('data/sync_run-2_events.tsv',delimiter='\\t')\n",
    "run1_data.head() # print the start of dataframe to see the format, we will drop the index column later when saving\n",
    "\n",
    "headers =  [\"onset\", \"duration\", \"weight\", \"stimulus\"]\n",
    "\n",
    "for sub in sub_list:\n",
    "    # use BIDS format naming\n",
    "    filename_run1 = os.path.join(data_dir,sub,'ses-movie','func',sub +'_task-sync_run-1_events.tsv')\n",
    "    filename_run2 = os.path.join(data_dir,sub,'ses-movie','func',sub +'_task-sync_run-2_events.tsv')\n",
    "    # save events\n",
    "    run1_data.to_csv(filename_run1, sep='\\t', index=False)\n",
    "    run2_data.to_csv(filename_run2, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "884cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly - we will use FSL BET to skull-strip our anatomical data - important for preprocessing stage!\n",
    "import nipype.interfaces.fsl as fsl\n",
    "bet = fsl.BET()\n",
    "for sub in sub_list:\n",
    "    bet.inputs.frac = 0.5 # fractional intensity threshold, 0.5 is default\n",
    "    bet.inputs.in_file = os.path.join(data_dir,sub,'ses-movie','anat',sub +'_T1w.nii.gz')\n",
    "    bet.inputs.out_file = os.path.join(data_dir,sub,'ses-movie','anat',sub +'_T1w_brain.nii.gz')\n",
    "    result = bet.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d22e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all preperations are done!\n"
     ]
    }
   ],
   "source": [
    "print('all preperations are done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
